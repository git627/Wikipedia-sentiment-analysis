{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6454e7f1-af05-4d31-ac81-9b69be3f13fb",
   "metadata": {},
   "source": [
    "In this example notebook, we will see how to perform sentiment analysis on multiple Wikipedia in a single job. First, we need to import the analyze function from wiki_sentiment_multi. We will be demonstrating two functions: analyze_list and analyze_csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc601e3-e461-4091-a7c0-db17caa7a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_sentiment_multi import analyze_list, analyze_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0891b1-8d11-4aae-a7ee-6287fdf199d6",
   "metadata": {},
   "source": [
    "First we will look at analyze_list. This will let us analyze multiple URLs specified as strings in a comma-separated list. This option is useful if you just want to do a quick analysis without needing any additional setup. Take a look at the inputs specified below. For our example, lets analyze a list of 3 famous singers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe41080-ec40-4175-87f6-250e783aac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=['https://en.wikipedia.org/wiki/Taylor_Swift',\n",
    "'https://en.wikipedia.org/wiki/Justin_Bieber',\n",
    "'https://en.wikipedia.org/wiki/Rihanna'] #List of URLs formatted as strings\n",
    "models=['bert','roberta','robertuito'] #Should be a string or list of strings. Available choices are 'bert','roberta','robertuito', and 'distilbert'.\n",
    "output_filename='singers_analysis.csv' #Filename for output csv file. Default is 'analysis.csv'\n",
    "output_path='path/to/my/file' #Filepath at which csv file is saved. If left unspecified, file will be saved in working location\n",
    "show_progress=True #Whether or not to display job progress. True by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd3e7b-cb99-4933-83b6-5ea37a19e324",
   "metadata": {},
   "source": [
    "Now we're ready to analyze! If show_progress is True, you will see job progress in terms of articles completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a8d80f-ad33-44ae-952c-c764317ba200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 articles analyzed\n",
      "2/3 articles analyzed\n",
      "3/3 articles analyzed\n"
     ]
    }
   ],
   "source": [
    "analyze_list(urls,models,output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71c5d8-5997-4b82-9591-fc4d3449690c",
   "metadata": {},
   "source": [
    "Job complete! The results that are recorded are exactly the same as the ones that we recorded for the single URL case. Next we will look at analyze_csv. This option is useful if you have a larger number of articles to analyze or have an existing table with custom metrics. The analyze_csv function will generate all the same results as before and simply append them to your existing table. The inputs for analyze_csv are mostly the same as for analyze_list, but you must specify the name of your starting .csv file. You can also optionally specify the input path of your .csv file and the name of column in your .csv file containing the article URLs. For this next example, let's perform sentiment analysis for some famous musicians, starting from a .csv file containing only their names and Wikipedia article URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1efcf1c5-7bc0-40b4-80c6-7624061f75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename='beatles.csv' #Filename of a UTF-8 encoded csv file that contains a column of URLs\n",
    "models=['bert','roberta','robertuito'] #Should be a string or list of strings. Available choices are 'bert','roberta','robertuito', and 'distilbert'.\n",
    "output_filename='beatles_analysis.csv' #Filename for output csv file. Default is 'analysis.csv'\n",
    "input_path='path/to/my/file' #Input filepath of csv file. If left unspecified, it is the current working directory\n",
    "output_path='path/to/my/file' #Filepath at which csv file is saved. If left unspecified, file will be saved in working location\n",
    "show_progress=True #Whether or not to display job progress. True by default\n",
    "url_col='URL' #Name of the column in input csv file containing the URLs. 'URL' by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3720db75-2da9-4734-937d-f0436cc3caed",
   "metadata": {},
   "source": [
    "Time to analyze!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6e17769-3cf2-45aa-878b-2c937444c4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 articles analyzed\n",
      "2/4 articles analyzed\n",
      "3/4 articles analyzed\n",
      "4/4 articles analyzed\n"
     ]
    }
   ],
   "source": [
    "analyze_csv(input_filename,models,output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b4a81-c8ae-4271-b311-dbf77b7848ac",
   "metadata": {},
   "source": [
    "This example has demonstrated two ways in which multiple Wikipedia articles can be analyzed via sentiment analysis with a single job. With these tools, you're ready to go perform your own study!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44cbf2a-d8d9-437d-8c2d-cb8bbf9e613b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
